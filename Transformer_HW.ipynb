{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeunghyunKim00/ML/blob/main/Transformer_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [STAT 38193-01] 2176074 김승현 Homework"
      ],
      "metadata": {
        "id": "tvuEhPE0XULK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az7nEU2Tg0z-",
        "outputId": "e0a036b0-14fe-4456-cb1f-a435211ccf1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./datasets/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:03<00:00, 48766150.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar-100-python.tar.gz to ./datasets/\n",
            "Files already downloaded and verified\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "## data loader\n",
        "\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = CIFAR100(root=path,train=True,transform=transform,download=True)\n",
        "test_data = CIFAR100(root=path,train=False,transform=transform,download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=0)\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=batch_size,shuffle=False,num_workers=0)\n",
        "\n",
        "input_shape = train_data[0][0].shape\n",
        "output_shape = len(train_data.classes)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Encoding**\n",
        "\n",
        "init부분에서는 input과 무관한 값들을 생성했다.\n",
        "\n",
        "pos로 0부터 데이터 길이만큼의 벡터를 생성하고, position값을 넣을 zero matrix를 self.pos_enc로 설정한다.\n",
        "\n",
        "for문을 이용하여 논문과 동일하게, sin,cos함수에 각각 대입한다.\n",
        "\n",
        "\n",
        "forward에서는 x가 input으로 들어왔을 때, 각각을 같은 device로 옮기고, positional embedding을 적용했다.\n"
      ],
      "metadata": {
        "id": "mANAW2ZSkZaR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKc2UoLwhBvW"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "# refer to Section 3.5 in the paper\n",
        "\n",
        "    def __init__(self,device,max_len=512,d_model=16):\n",
        "        super().__init__()\n",
        "        # fill out here\n",
        "        # how should we fill out self.pos_enc?\n",
        "        pos =  torch.arange(0, max_len).to(device) # 512-1\n",
        "        self.pos_enc = torch.zeros(max_len,d_model,requires_grad=False).to(device) # 512-16 shape zero matrix\n",
        "        for i in range(d_model):\n",
        "            if i%2 == 0:\n",
        "                self.pos_enc[:,i] = torch.sin(pos/(10000**(i/d_model)))\n",
        "            else:\n",
        "                self.pos_enc[:,i] = torch.cos(pos/(10000**((i-1)/d_model)))\n",
        "\n",
        "    def forward(self,x):\n",
        "        # fill out here\n",
        "        \"\"\"\n",
        "        x: transformed input embedding where x.shape = [batch_size, seq_len, data_dim]\n",
        "        \"\"\"\n",
        "        pos_emb = x + self.pos_enc\n",
        "\n",
        "        return pos_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scale Dot Product Attention**\n",
        "\n",
        "forward에서 input으로 mask가 있는 경우와 없는 경우로 나누어서 encoder layer와 decoder layer에서 다르게 작동하도록 하였다.\n",
        "\n",
        "공통적으로 attention score는 query와 key의 matrix multiplication에 model dimension의 squreroot값으로 나눈 꼴을 갖는다. 이 때, q,k는 각각 4dim이므로 matrix multiplication을 위해서 key부분을 transpose하여 곱했다.\n",
        "\n",
        "**[CIFAR100 data shape]**\n",
        "\n",
        "100-4-512-4 -> 100-4-4-512\n",
        "\n",
        "100-4-512-4 @ 100-4-4-512 -> 100-4-512-512 : attention score shape\n",
        "\n",
        "아래 Masking을 이용해서 생성한 mask가 None이 아니라면, attention score의 일부를 가려야한다.\n",
        "masked fill을 이용하는 것은 https://github.com/hyunwoongko/transformer 의 1.3 Scale Dot Product Attention 부분 코드를 참고했다.\n",
        "\n",
        "masking한 attention score에 softmax를 씌우고, value와 matrix multiplication을 한 attention value를 return한다."
      ],
      "metadata": {
        "id": "vJjt7ZqftNXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAwIc_4ihD-a"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "# refer to Section 3.2.1 and Fig 2 (left) in the paper\n",
        "\n",
        "    def __init__(self,d_model=16):\n",
        "        super().__init__()\n",
        "        # there is nothing to do here\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self,q,k,v,mask=None):\n",
        "        # fill out here\n",
        "        # compute attention value based on transformed query, key, value where mask is given conditionally\n",
        "        \"\"\"\n",
        "        q, k, v = transformed query, key, value\n",
        "        q.shape, k.shape, v.shpae = [batch_size, num_head, seq_len, d_k=d_model/num_head]\n",
        "        mask = masking matrix, if the index has value False, kill the value; else, leave the value\n",
        "        \"\"\"\n",
        "        attention_score = torch.matmul(q,k.transpose(-2,-1)) # 100-4-512-4 -> 100-4-4-512 -> matmul 100-4-512-512\n",
        "        attention_score = attention_score/(self.d_model**(0.5)) # 100-4-512-512\n",
        "\n",
        "        if mask != None:\n",
        "            # .masked_fill : 0인 것들을 -1e10으로 바꾸고, 고정하고자 하는 것을 1로 둔다.\n",
        "            attention_score = attention_score.masked_fill(mask == 0, -1e10)\n",
        "            # attention_score shape : 100-4-512-512\n",
        "\n",
        "        attention_score = torch.softmax(attention_score, dim = -1) # d_ff로 softmax\n",
        "\n",
        "        attention_value = torch.matmul(attention_score ,v)\n",
        "\n",
        "        return attention_value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MultiHeadAttention**\n",
        "\n",
        "init\n",
        "\n",
        "맨 앞단의 fully connected linear연산을 위한 self.lin_q,k,v를 만들고, 위의 ScaleDotProductAttention class를 self.attention으로, 마지막에 넣어주는 linear transformation에 해당하는 self.lin_o를 만들어둔다.\n",
        "\n",
        "forward\n",
        "\n",
        "input으로 들어온 q,k,v를 각각 변환한 뒤, shape을 100-4-512-4로 만들어주는 reshape을 한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "qaqW1yIZ22dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 수정한 부분\n",
        "\n",
        "transpose에서 100-512-16 -> 100-4-512-4를 하려면 그냥 reshape 써버린다면, 512에 있던 2^2가 들어가게 되어서 원하는 변환이 아님. 따라서 100-512-4-4로 reshape을 하고, 100-4-512-4로 transpose를 해야함.\n",
        "\n",
        "concatenate도 마찬가지 방법으로"
      ],
      "metadata": {
        "id": "9f3KViQqsPth"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnOiY0YDhV61"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "# refer to Section 3.2.2 and Fig 2 (right) in the paper\n",
        "    def __init__(self,d_model=16,num_head=4):\n",
        "        super().__init__()\n",
        "        # fill out the rest\n",
        "        # refer to\n",
        "\n",
        "        assert d_model % num_head == 0, \"check if d_model is divisible by num_head\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_head = num_head\n",
        "        self.d_k = d_model//num_head\n",
        "\n",
        "        # 맨 앞단의 fully connected linear 연산\n",
        "        self.lin_q = nn.Linear(self.d_model, self.d_model)\n",
        "        self.lin_k = nn.Linear(self.d_model, self.d_model)\n",
        "        self.lin_v = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(d_model = d_model)\n",
        "\n",
        "        self.lin_o = nn.Linear(self.d_model,self.d_model)\n",
        "\n",
        "    def forward(self,q,k,v,mask=None):\n",
        "        # fill out here\n",
        "        # compute multi-head attention value\n",
        "        # here, query, key, value are pre-transformed, so you need to transfrom them in this module\n",
        "        \"\"\"\n",
        "        q, k, v = pre-transformed query, key, value\n",
        "        q.shape, k.shape, v.shpae = [batch_size, seq_len, d_model]\n",
        "        mask = masking matrix, if the index has value False, kill the value; else, leave the value\n",
        "        \"\"\"\n",
        "        Q = self.lin_q(q) # 100-512-16\n",
        "        K = self.lin_k(k)\n",
        "        V = self.lin_v(v)\n",
        "\n",
        "        # [batch_size, num_heads, seq_len, d_k] -> scaleddotproductattention 사용하기 위해서\n",
        "        Q = Q.reshape(batch_size, -1, self.num_head, self.d_k).transpose(1,2) #Q.reshape(batch_size, self.num_head, -1, self.d_k) # 100-4-512-4 => dim 섞임\n",
        "        K = K.reshape(batch_size, -1, self.num_head, self.d_k).transpose(1,2) #K.reshape(batch_size, self.num_head, -1, self.d_k)\n",
        "        V = V.reshape(batch_size, -1, self.num_head, self.d_k).transpose(1,2) #V.reshape(batch_size, self.num_head, -1, self.d_k)\n",
        "\n",
        "        attention = self.attention(Q,K,V,mask = mask) # attention value by Scaled dot-product attention\n",
        "\n",
        "        # concat_attention = attention.reshape(batch_size, -1, self.d_model) # concat 원래 모양으로\n",
        "        concat_attention = attention.transpose(1,2).reshape(batch_size, -1, self.d_model)\n",
        "        output = self.lin_o(concat_attention)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Feed Forward**\n",
        "\n",
        "Positional Feed Forward는 d_model -> d_ff -> d_model nn.linear, relu를 사용하여 작성하였다."
      ],
      "metadata": {
        "id": "nYWFw_bXP47I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-JRc6whe9H"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForwardNetwork(nn.Module):\n",
        "# refer to Section 3.3 in the paper\n",
        "# do not use torch.nn.Conv1d\n",
        "\n",
        "    def __init__(self,d_model=16,d_ff=32):\n",
        "        super().__init__()\n",
        "        # fill out here\n",
        "        self.W1 = nn.Linear(d_model,d_ff)\n",
        "        self.W2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        # fill out here\n",
        "        temp = self.relu(self.W1(x))\n",
        "        output = self.W2(temp)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Masking**\n",
        "\n",
        "Masking은 lower triangle matrix를 data의 길이에 맞게 생성하여 이후 matrix에서 1인 값만 남기고 0인 값은 masking을 하는 연산을 진행한다.\n",
        "\n",
        "사용하는 데이터의 길이가 일정하므로, padding은 따로 진행하지 않았다.\n"
      ],
      "metadata": {
        "id": "Xm9Eaa6djtcy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQh2s15Mg-t9"
      },
      "outputs": [],
      "source": [
        "class Masking(nn.Module):\n",
        "\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x_len = x.shape[1]\n",
        "        x_mask = torch.tril(torch.ones((x_len, x_len)))\n",
        "        x_mask = x_mask.view(x_len, x_len)\n",
        "        x_mask = x_mask.to(self.device)\n",
        "\n",
        "        return x_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LayerNormaliation**\n",
        "\n",
        "https://github.com/hyunwoongko/transformer\n",
        "Layer Norm을 참고했습니다."
      ],
      "metadata": {
        "id": "vbEeNrKHMoa_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l23QSZ3hh2X"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "# do not use torch.nn.LayerNorm\n",
        "\n",
        "    def __init__(self,device,d_model=16,eps=1e-5):\n",
        "        super().__init__()\n",
        "        # fill out here\n",
        "        self.eps = eps\n",
        "        self.d_model = d_model\n",
        "        # hyunwoongko\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model),requires_grad = True)\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model), requires_grad = True)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # fill out here\n",
        "        # feedforward output으로 나온 값 : 100-512-16\n",
        "        mean = x.mean(-1, keepdim = True)\n",
        "        var = x.var(-1, unbiased = False, keepdim = True) # 최근 torch.var에서는 unbiased 없는 것 같음.-> 이전ver.\n",
        "\n",
        "        normed = (x-mean)/torch.sqrt(var+self.eps)\n",
        "\n",
        "        normed = self.gamma*normed + self.beta\n",
        "\n",
        "        return normed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EncoderLayer**\n",
        "\n",
        "앞서 만든 multihead attention, feed forward, layernorm를 각각 init에서 인스턴스화 하고, encoder block 처음에 linear를 통해서 input값을 q,k,v로 만들어주는 nn.Linear도 3개 생성한다.\n",
        "\n",
        "foward는 init에서 만들어둔 초기화된 인스턴스와 nn.Linear에 대해서 연산을 진행한다. dropout은 multihead attention, positionwise feed forwared network의 뒤에 넣어주었다."
      ],
      "metadata": {
        "id": "wUZ-uzYCQrNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 수정한 부분\n",
        "1. multihead attention 이전의 linear연산 없앰 -> parameter수 2개씩은 없어질 듯\n",
        "2. dropout은 parameter없어서 하나만 있어도 됨."
      ],
      "metadata": {
        "id": "OiS6lw9Mr_I9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbLER-TEhjTj"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# this is a single encoder block consists of the following\n",
        "# multi-head attention, positionwise feed forward network, residual connections, layer normalizations\n",
        "\n",
        "    def __init__(self,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        # fill out here\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.multiheadattention = MultiHeadAttention(d_model = d_model, num_head = num_head)\n",
        "        self.ffn = PositionwiseFeedForwardNetwork(d_model, d_ff)\n",
        "        self.layernorm1 = LayerNormalization(d_model)\n",
        "        self.layernorm2 = LayerNormalization(d_model)\n",
        "\n",
        "        # encoder block 처음에 linear 연산\n",
        "\n",
        "\n",
        "    def forward(self,enc):\n",
        "        # fill out here\n",
        "\n",
        "        #hidden = enc + self.multiheadattention(Q,K,V) # 100-512-16\n",
        "        hidden = self.multiheadattention(enc,enc,enc,None)\n",
        "        hidden = enc + self.dropout(hidden)\n",
        "        hidden = self.layernorm1(hidden)\n",
        "\n",
        "        ffn_hid = self.dropout(self.ffn(hidden))\n",
        "        hidden = hidden + ffn_hid\n",
        "        output = self.layernorm2(hidden)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DecoderLayer**\n",
        "\n",
        "DecoderLayer는 decoder의 input으로 들어오는 지금까지 생성한 값에 대한 attention과, encoder의 output과 만들어진 query값의 attention 두 개의 multihead attention이 있다.\n",
        "\n",
        "attention과 ffn 뒤에 layernorm이 오도록 만들었으므로, init에서 3개의 layernorm을 정의 하고, decoder input에 대한 linear 연산과, encoder, decoder attention에 대한 linear도 함께 정의한다.\n",
        "\n",
        "encoder와 유사한 방법으로 self-masked multihead attention, multihead attention, feedforward를 순차적으로 계산한다."
      ],
      "metadata": {
        "id": "BsWRomXpSN4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 수정한 부분\n",
        "1. layer에서는 multihead attention에서 linear transfomation을 진행하므로 여기서는 할 필요가 없다. 그래서 nn.linear multihead attention 앞에 있던 것들 모두 없앰.\n",
        "\n",
        "2. dropout은 parameter가 없어서 하나만 만들어도 됨."
      ],
      "metadata": {
        "id": "-aa7jWtlriA1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb6W-rcxhjyc"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# this is a single decoder block consists of the following\n",
        "# masked multi-head attention, multi-head attention, positionwise feed forward network, residual connections, layer normalizations\n",
        "\n",
        "    def __init__(self, d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        # fill out here\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.dec_in_mha = MultiHeadAttention(d_model = d_model, num_head = num_head)\n",
        "        self.enc_dec_mha = MultiHeadAttention(d_model = d_model, num_head = num_head)\n",
        "        self.ffn = PositionwiseFeedForwardNetwork(d_model, d_ff)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(d_model)\n",
        "        self.layernorm2 = LayerNormalization(d_model)\n",
        "        self.layernorm3 = LayerNormalization(d_model)\n",
        "\n",
        "\n",
        "    def forward(self,enc_output,dec,dec_mask):\n",
        "        # fill out here\n",
        "        #dec_in_qkv = [linear(dec) for linear in self.lin_qkv]\n",
        "\n",
        "        temp = self.dec_in_mha(dec,dec,dec,mask = dec_mask)\n",
        "        temp = self.dropout(temp)\n",
        "        temp = temp + dec\n",
        "        dec_query = self.layernorm1(temp)\n",
        "\n",
        "\n",
        "        hidden = self.enc_dec_mha(dec_query, enc_output, enc_output)\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden = dec_query + hidden\n",
        "        hidden = self.layernorm2(hidden)\n",
        "\n",
        "        ffn_hidden = self.ffn(hidden)\n",
        "        ffn_hidden = self.dropout(ffn_hidden)\n",
        "        hidden = hidden + ffn_hidden\n",
        "        output = self.layernorm3(hidden)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**\n",
        "\n",
        "num_layer의 횟수만큼 encoder block을 반복한다. 이 때, encoderblock을 num_layer만큼 init에서 만들어서 진행해야한다.\n",
        "\n",
        "self.lin_layer로 input embedding을 생성하고, positional encoding으로 encoder block 이전 과정을 포함해야한다.\n",
        "\n",
        "for-loop을 이용하여 num_layer의 iteration을 구현하였다."
      ],
      "metadata": {
        "id": "79dfPmEfT6cb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBCCQX7thuM-"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# this is a whole encoder, i.e., the left side of Figure 1, consists of the following as well\n",
        "# input embedding, positional encoding\n",
        "\n",
        "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_layer = nn.Linear(input_dim, d_model)  # transform the input into the hidden dim with single linear transformation\n",
        "        # fill out here\n",
        "        self.num_layer = num_layer\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(device, max_len =  max_len, d_model = d_model)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model = d_model, num_head = num_head, d_ff = d_ff, drop_prob = drop_prob).to(device) for _ in range(num_layer)])\n",
        "\n",
        "    def forward(self,x):\n",
        "        # fill out here\n",
        "\n",
        "        enc = self.lin_layer(x)\n",
        "        hidden = self.positional_encoding(enc)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            hidden = layer(hidden)\n",
        "\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**\n",
        "\n",
        "encoder와 마찬가지 방법으로 decoder block을 num_layer만큼 iteration 한다.\n",
        "\n",
        "Decoder block 이전의 embedding과 positional encoding을 포함하고 for-loop으로 num_layer만큼 iteration한다.\n",
        "\n",
        "마지막 output은 linear로 input과 같은 dimension이 되도록 변환한다.\n",
        "\n",
        "아래 training에서 loss가 nn.BCEWithLogitsLoss(reduction='sum')를 사용하므로 logit의 상태로 output을 반환해야 한다. 따라서 마지막에 softmax를 포함하지 않는다."
      ],
      "metadata": {
        "id": "56kVTFqhUjji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9vF_dB7hzKs"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# this is a whole decoder, i.e., the left side of Figure 1, consists of the following as well\n",
        "# input embedding, positional encoding, linear classifier\n",
        "\n",
        "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        # fill out here\n",
        "        # self.encoder = Encoder(input_dim, num_layer, max_len, d_model, num_head, d_ff, drop_prob)\n",
        "\n",
        "        self.num_layer = num_layer\n",
        "\n",
        "        self.lin_layer = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(device = device,max_len = max_len, d_model = d_model)\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model,num_head,d_ff,drop_prob) for _ in range(num_layer)]).to(device)\n",
        "\n",
        "        self.o_lin = nn.Linear(d_model, input_dim)\n",
        "\n",
        "    def forward(self,enc_output,y,y_mask):\n",
        "        # fill out here\n",
        "\n",
        "        dec = self.lin_layer(y)\n",
        "        hidden = self.positional_encoding(dec)\n",
        "\n",
        "        for layer in self.decoder_layers:\n",
        "            hidden = layer(enc_output, hidden, y_mask)\n",
        "\n",
        "        output = self.o_lin(hidden) # 100-512-3\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer**\n",
        "\n",
        "Transformer는 위에서 만든 Encoder, Decoder를 합치고, masking을 만들어 decoder에 대입하는 연산을 진행한다."
      ],
      "metadata": {
        "id": "93CB6Uw0Wc5q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6hdLm01h2bl"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "# refer to Section 3.1 and Figure 1 in the paper\n",
        "# sum up encoder and decoder\n",
        "\n",
        "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
        "        super().__init__()\n",
        "        # fill out here\n",
        "        self.encoder = Encoder(device,input_dim,num_layer,max_len,d_model,num_head,d_ff,drop_prob)\n",
        "        self.decoder = Decoder(device,input_dim,num_layer,max_len,d_model,num_head,d_ff,drop_prob)\n",
        "        self.masking = Masking(device)\n",
        "\n",
        "    def forward(self,x,y):\n",
        "        # fill out here\n",
        "        enc_output = self.encoder(x)\n",
        "        mask_y = self.masking(y)\n",
        "        dec_output = self.decoder(enc_output,y,mask_y)\n",
        "\n",
        "        return dec_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train & Test**\n",
        "\n",
        "num_param: 30563\n",
        "\n",
        "Epoch 0 Train: **0.632709** w/ Learning Rate: 0.00049\n",
        "\n",
        "Epoch 0 Test: **0.564997**\n",
        "\n",
        "Epoch 29 Train: **0.540526** w/ Learning Rate: 0.00204\n",
        "\n",
        "Epoch 29 Test: **0.539083**"
      ],
      "metadata": {
        "id": "OMRcGQiwW1XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScheduledOptimizer:\n",
        "\n",
        "    def __init__(self,optimizer,d_model=16,warmup_steps=4000):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def update_parameter_and_learning_rate(self):\n",
        "        self.optimizer.step()\n",
        "        self.step_num += 1\n",
        "        self.lr = self.d_model**(-.5) * min(self.step_num**(-.5),self.step_num*self.warmup_steps**(-1.5))\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.lr\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Transformer(device=device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=64,drop_prob=.1).to(device)\n",
        "loss = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(),betas=(.9,.98),eps=1e-9)\n",
        "scheduled_optimizer = ScheduledOptimizer(optimizer,d_model=16)\n",
        "\n",
        "\n",
        "num_epoch = 15\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"num_param:\", total_params)\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    ## train\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "\n",
        "        image = image.reshape(-1,3,1024).transpose(1,2)  # 1024로 변환\n",
        "        x, y = image[:,:512,:].to(device), image[:,512:,:].to(device) # 앞의 512를 input, 뒤의 512를 output (want)\n",
        "\n",
        "        # batch = 100\n",
        "        # y_ : 100-1-3 size zero + dim=1에서 마지막 값 하나 없애고 붙이기\n",
        "        y_ = torch.zeros([batch_size,1,3],requires_grad=False).to(device)\n",
        "        y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "        # 앞에 1개는 0으로 만들고 -> initial output(마지막 output 직전에..)\n",
        "        # decoder에 input으로 넣을 값은 마지막을 제거해야 함. : last input...?\n",
        "\n",
        "        logit = model.forward(x,y_)\n",
        "        cost = loss(logit,y)/(3*512)\n",
        "\n",
        "        total_loss += cost.item()\n",
        "\n",
        "        scheduled_optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        scheduled_optimizer.update_parameter_and_learning_rate()\n",
        "\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.6f w/ Learning Rate: %.5f\"%(i,ave_loss,scheduled_optimizer.lr))\n",
        "\n",
        "    ## test\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(test_loader):\n",
        "\n",
        "            image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "            x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
        "\n",
        "            y_ = torch.zeros([batch_size,1,3],requires_grad=False).to(device)\n",
        "            y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "            logit = model.forward(x,y_)\n",
        "            cost = loss(logit, y)/(3*512)\n",
        "\n",
        "            total_loss += cost.item()\n",
        "\n",
        "    ave_loss = total_loss/len(test_data)\n",
        "    test_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"Epoch %d Test: %.6f\"%(i,ave_loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdogwyEIDxhh",
        "outputId": "e1abaadd-84f2-4278-beb6-836dc23acdc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_param: 23219\n",
            "\n",
            "Epoch 0 Train: 0.613294 w/ Learning Rate: 0.00049\n",
            "Epoch 0 Test: 0.565276\n",
            "\n",
            "Epoch 1 Train: 0.560341 w/ Learning Rate: 0.00099\n",
            "Epoch 1 Test: 0.555663\n",
            "\n",
            "Epoch 2 Train: 0.556542 w/ Learning Rate: 0.00148\n",
            "Epoch 2 Test: 0.554102\n",
            "\n",
            "Epoch 3 Train: 0.555291 w/ Learning Rate: 0.00198\n",
            "Epoch 3 Test: 0.553805\n",
            "\n",
            "Epoch 4 Train: 0.554372 w/ Learning Rate: 0.00247\n",
            "Epoch 4 Test: 0.552239\n",
            "\n",
            "Epoch 5 Train: 0.552991 w/ Learning Rate: 0.00296\n",
            "Epoch 5 Test: 0.551938\n",
            "\n",
            "Epoch 6 Train: 0.551559 w/ Learning Rate: 0.00346\n",
            "Epoch 6 Test: 0.549474\n",
            "\n",
            "Epoch 7 Train: 0.549977 w/ Learning Rate: 0.00395\n",
            "Epoch 7 Test: 0.547790\n",
            "\n",
            "Epoch 8 Train: 0.548020 w/ Learning Rate: 0.00373\n",
            "Epoch 8 Test: 0.545080\n",
            "\n",
            "Epoch 9 Train: 0.546260 w/ Learning Rate: 0.00354\n",
            "Epoch 9 Test: 0.542930\n",
            "\n",
            "Epoch 10 Train: 0.543705 w/ Learning Rate: 0.00337\n",
            "Epoch 10 Test: 0.540571\n",
            "\n",
            "Epoch 11 Train: 0.542127 w/ Learning Rate: 0.00323\n",
            "Epoch 11 Test: 0.539427\n",
            "\n",
            "Epoch 12 Train: 0.541049 w/ Learning Rate: 0.00310\n",
            "Epoch 12 Test: 0.538388\n",
            "\n",
            "Epoch 13 Train: 0.540240 w/ Learning Rate: 0.00299\n",
            "Epoch 13 Test: 0.538797\n",
            "\n",
            "Epoch 14 Train: 0.539856 w/ Learning Rate: 0.00289\n",
            "Epoch 14 Test: 0.537830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고한 페이지\n",
        "\n",
        "https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "https://github.com/hyunwoongko/transformer/tree/master\n",
        "\n",
        "https://paul-hyun.github.io/transformer-02/"
      ],
      "metadata": {
        "id": "6LL32xwPY2ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_epoch):\n",
        "\n",
        "    ## train\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "\n",
        "        image = image.reshape(-1,3,1024).transpose(1,2)  # 1024로 변환\n",
        "        x, y = image[:,:512,:].to(device), image[:,512:,:].to(device) # 앞의 512를 input, 뒤의 512를 output (want)\n",
        "\n",
        "        # batch = 100\n",
        "        # y_ : 100-1-3 size zero + dim=1에서 마지막 값 하나 없애고 붙이기\n",
        "        y_ = torch.zeros([batch_size,1,3],requires_grad=False).to(device)\n",
        "        y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "        # 앞에 1개는 0으로 만들고 -> initial output(마지막 output 직전에..)\n",
        "        # decoder에 input으로 넣을 값은 마지막을 제거해야 함. : last input...?\n",
        "\n",
        "        logit = model.forward(x,y_)\n",
        "        cost = loss(logit,y)/(3*512)\n",
        "\n",
        "        total_loss += cost.item()\n",
        "\n",
        "        scheduled_optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        scheduled_optimizer.update_parameter_and_learning_rate()\n",
        "\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.6f w/ Learning Rate: %.5f\"%(i,ave_loss,scheduled_optimizer.lr))\n",
        "\n",
        "    ## test\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(test_loader):\n",
        "\n",
        "            image = image.reshape(-1,3,1024).transpose(1,2)\n",
        "            x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
        "\n",
        "            y_ = torch.zeros([batch_size,1,3],requires_grad=False).to(device)\n",
        "            y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
        "\n",
        "            logit = model.forward(x,y_)\n",
        "            cost = loss(logit, y)/(3*512)\n",
        "\n",
        "            total_loss += cost.item()\n",
        "\n",
        "    ave_loss = total_loss/len(test_data)\n",
        "    test_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"Epoch %d Test: %.6f\"%(i+15,ave_loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf5Hnq6I2Qzc",
        "outputId": "b62e64e9-a58b-4cd6-d310-d20852c2c4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 Train: 0.539596 w/ Learning Rate: 0.00280\n",
            "Epoch 15 Test: 0.537779\n",
            "\n",
            "Epoch 1 Train: 0.539368 w/ Learning Rate: 0.00271\n",
            "Epoch 16 Test: 0.537460\n",
            "\n",
            "Epoch 2 Train: 0.539189 w/ Learning Rate: 0.00264\n",
            "Epoch 17 Test: 0.537362\n",
            "\n",
            "Epoch 3 Train: 0.539071 w/ Learning Rate: 0.00256\n",
            "Epoch 18 Test: 0.537162\n",
            "\n",
            "Epoch 4 Train: 0.538988 w/ Learning Rate: 0.00250\n",
            "Epoch 19 Test: 0.537236\n",
            "\n",
            "Epoch 5 Train: 0.538910 w/ Learning Rate: 0.00244\n",
            "Epoch 20 Test: 0.537067\n",
            "\n",
            "Epoch 6 Train: 0.538822 w/ Learning Rate: 0.00238\n",
            "Epoch 21 Test: 0.536994\n",
            "\n",
            "Epoch 7 Train: 0.538755 w/ Learning Rate: 0.00233\n",
            "Epoch 22 Test: 0.536921\n",
            "\n",
            "Epoch 8 Train: 0.538689 w/ Learning Rate: 0.00228\n",
            "Epoch 23 Test: 0.536911\n",
            "\n",
            "Epoch 9 Train: 0.538639 w/ Learning Rate: 0.00224\n",
            "Epoch 24 Test: 0.536851\n",
            "\n",
            "Epoch 10 Train: 0.538583 w/ Learning Rate: 0.00219\n",
            "Epoch 25 Test: 0.537087\n",
            "\n",
            "Epoch 11 Train: 0.538540 w/ Learning Rate: 0.00215\n",
            "Epoch 26 Test: 0.536792\n",
            "\n",
            "Epoch 12 Train: 0.538501 w/ Learning Rate: 0.00211\n",
            "Epoch 27 Test: 0.536806\n",
            "\n",
            "Epoch 13 Train: 0.538460 w/ Learning Rate: 0.00208\n",
            "Epoch 28 Test: 0.536879\n",
            "\n",
            "Epoch 14 Train: 0.538426 w/ Learning Rate: 0.00204\n",
            "Epoch 29 Test: 0.536709\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}